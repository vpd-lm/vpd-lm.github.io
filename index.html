<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="VPD-LM: An end-to-end Vision-Language Model for Autonomous Driving using object-centric visual prompts and chain-of-thought reasoning for enhanced safety assessment, perception, and planning.">
  <meta property="og:title" content="VPD-LM: Improving Driving Safety Assessment via Object-Level Visual Prompting in Vision-Language Models"/>
  <meta property="og:description" content="VPD-LM addresses limitations in VLM-based autonomous driving by incorporating object-centric, video-based visual prompts and chain-of-thought reasoning to enhance safety, interpretability, and robustness."/>
  <meta property="og:url" content="YOUR_WEBSITE_URL_HERE"/>
  <meta property="og:image" content="static/image/vpd_lm_banner.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="VPD-LM: Improving Driving Safety Assessment via Object-Level Visual Prompting in Vision-Language Models">
  <meta name="twitter:description" content="VPD-LM addresses limitations in VLM-based autonomous driving by incorporating object-centric, video-based visual prompts and chain-of-thought reasoning to enhance safety, interpretability, and robustness.">
  <meta name="twitter:image" content="static/images/vpd_lm_twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Autonomous Driving, Vision-Language Models, Safety Assessment, Visual Prompting, Risk Assessment, VPD-LM, NeurIPS">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>VPD-LM: Driving Safety Assessment</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* Base font */
    body { font-size: 0.85rem; }

    /* Clear hierarchy */
    h1.title.is-1.publication-title { font-size: 2.3rem; }
    h2.title.is-3.has-text-centered { font-size: 1.8rem; margin-bottom: .6rem; }
    h3.subtitle.is-size-4-tablet    { font-size: 1.25rem; }
    h4.title.is-4.has-text-centered { font-size: 1.3rem; }

    /* Paragraph & code */
    p.is-size-5, .content p { font-size: 0.5rem; line-height: 1.45; }
    pre code                { font-size: 0.4rem; }

    /* Layout width */
    .container.is-max-desktop { max-width: 1200px; }

    hr.section-divider { margin: .4rem 0 1.2rem 0; border: none; height: 1px; background: #e0e0e0; }
  </style>
</head>
<body>

<!-- ────────── HERO TITLE ────────── -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            VPD-LM: Improving Driving Safety Assessment via Object-Level Visual Prompting in Vision-Language Models
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Anonymous Author(s)</span>
          </div>
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <a href="VPD_LM (5).pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="ai ai-arxiv"></i></span><span>Paper PDF</span>
          </a>
          <a href="#" target="_blank" class="external-link button is-normal is-rounded is-dark">
            <span class="icon"><i class="fab fa-github"></i></span><span>Code (Coming Soon)</span>
          </a>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ────────── ABSTRACT ────────── -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="content has-text-justified">
        <h2 class="title is-3 has-text-centered">Abstract</h2>
        <hr class="section-divider">
        <p class="is-size-5">
          Vision-Language Models (VLMs) have demonstrated remarkable visual reasoning capabilities, positioning them as promising solutions for a wide range of autonomous driving (AD) tasks. Unlike traditional expert-driven AD models, which often operate as opaque black boxes, VLMs generate natural language explanations for their decisions, offering inherent interpretability. This interpretability enables explicit reasoning about driving safety and facilitates the identification of rare or high-risk scenarios that conventional systems may overlook. However, existing VLM-based frameworks often struggle with grounding and tracking specific objects over time—a limitation that undermines robust decision-making in dynamic, real-world environments.
          In this work, we introduce <em><b>VPD-LM</b></em>, an end-to-end Vision-Language Model for Autonomous Driving that addresses these challenges through object-centric, video-based visual prompts and chain-of-thought (CoT) reasoning. <em><b>VPD-LM</b></em> processes raw camera inputs and sensor metadata to generate interpretable natural language outputs for core driving tasks, including planning, perception, and safety assessment.
          To rigorously evaluate safety reasoning, we go beyond standard nuScenes-based benchmarks by constructing a novel Driving Safety Benchmark. We extract approximately 3,000 long-tail scenarios with potential collision risks from the larger nuPlan dataset using our proposed Road Graph Occupancy Detection method. From these, 600 representative scenarios are manually labeled into three risk categories—low, medium, and high—creating a challenging and fine-grained testbed for risk-aware driving models.
          Empirical results demonstrate that <em><b>VPD-LM</b></em> achieves strong performance in motion planning and threat assessment while generalizing effectively to complex and previously unseen scenarios. Our work highlights the potential of VLM-driven reasoning to significantly enhance safety, interpretability, and robustness in next-generation autonomous driving systems.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- ────────── OUR APPROACH ────────── -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Our Approach</h2>
      <hr class="section-divider">

      <div id="results-carousel-approach" class="carousel results-carousel" data-interval="4500">

        <!-- 1 -->
        <div class="item">
          <h3 class="subtitle is-size-4-tablet has-text-weight-bold has-text-centered has-background-info-light pt-3 pb-3">
            1. Object-Centric Visual Prompting &amp; Annotation
          </h3>
          <div class="columns is-vcentered">
            <div class="column is-one-third">
              <img src="static/images/vpd_lm_concept_visual_prompting.png" alt="Visual Prompting"
                   style="width:100%;border:1px solid #ccc;">
            </div>
            <div class="column">
              <p class="is-size-5">
                Detection models (e.g., DINO, YOLO-World) mark key road agents; SAM-2 tracks them across frames,
                producing masks and IDs that become visual prompts guiding the VLM’s attention.
              </p>
            </div>
          </div>
        </div>

        <!-- 2 -->
        <div class="item">
          <h3 class="subtitle is-size-4-tablet has-text-weight-bold has-text-centered has-background-info-light pt-3 pb-3">
            2. Mining Long-Tail Risky Scenarios
          </h3>
          <div class="columns is-vcentered">
            <div class="column is-one-third">
              <img src="static/images/vpd_lm_concept_risky_scenarios.png" alt="Risky Scenarios"
                   style="width:100%;border:1px solid #ccc;">
            </div>
            <div class="column">
              <p class="is-size-5">
                A rule-based Road-Graph Occupancy method simulates short-horizon actions; scenarios with overlapping
                occupancies are flagged as risky, forming our new benchmark.
              </p>
            </div>
          </div>
        </div>

        <!-- 3 -->
        <div class="item">
          <h3 class="subtitle is-size-4-tablet has-text-weight-bold has-text-centered has-background-info-light pt-3 pb-3">
            3. Structured Grounded Reasoning with VLMs
          </h3>
          <div class="columns is-vcentered">
            <div class="column is-one-third">
              <img src="static/images/vpd_lm_concept_reasoning_flow.png" alt="Reasoning Flow"
                   style="width:100%;border:1px solid #ccc;">
            </div>
            <div class="column">
              <p class="is-size-5">
                Hierarchical chain-of-thought: Scene → Crucial Objects → Task-specific module
                (Prediction / Planning / Risk Assessment), producing interpretable outputs.
              </p>
            </div>
          </div>
        </div>

      </div><!-- /carousel -->
    </div>
  </div>
</section>

<!-- ────────── RESULTS ────────── -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Demonstration &amp; Results</h2>
      <hr class="section-divider">

      <div class="columns is-multiline">

        <div class="column is-one-third">
          <div class="box">
            <h4 class="title is-4 has-text-centered">Enhanced Perception, Prediction &amp; Planning</h4>
            <img src="static/images/vpd_lm_capability_ppp.png" alt="PPP"
                 style="width:100%;margin-bottom:1em;border:1px solid #ccc;">
            <p class="is-size-5">
              VPD-LM outputs detailed perception, intent prediction, and ego-vehicle plans in natural language.
            </p>
          </div>
        </div>

        <div class="column is-one-third">
          <div class="box">
            <h4 class="title is-4 has-text-centered">Superior Safety Assessment<br>(Driving Risky Benchmark)</h4>
            <img src="static/images/vpd_lm_results_safety_benchmark.png" alt="Safety Benchmark"
                 style="width:100%;margin-bottom:1em;border:1px solid #ccc;">
            <p class="is-size-5">
              On 600 nuPlan risk cases, VPD-R1 hits <b>92.28&nbsp;%</b> accuracy, beating GPT-40 (60&nbsp;%) and
              InternVL3 (69&nbsp;%).
            </p>
          </div>
        </div>

        <div class="column is-one-third">
          <div class="box">
            <h4 class="title is-4 has-text-centered">Broad Task Generalization<br>(DriveBench)</h4>
            <img src="static/images/vpd_lm_results_drivebench.png" alt="DriveBench"
                 style="width:100%;margin-bottom:1em;border:1px solid #ccc;">
            <p class="is-size-5">
              VPD-R1 tops DriveBench in Perception (52.69) and Planning (60.67), showing good transfer.
            </p>
          </div>
        </div>

      </div><!-- /columns -->
    </div>
  </div>
</section>

<!-- ────────── BIBTEX ────────── -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">BibTeX</h2>
    <hr class="section-divider">
<pre><code>@inproceedings{vpd_lm_neurips2025,
  title  = {Improving Driving Safety Assessment via Object-Level Visual Prompting in Vision-Language Models},
  author = {Anonymous Author(s)},
  booktitle = {Submitted to Conference on Neural Information Processing Systems (NeurIPS)},
  year   = {2025}
}</code></pre>
  </div>
</section>

<!-- ────────── FOOTER ────────── -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="color:gray;font-size:9.9px;">
            Page built with
            <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">
              Academic Project Page Template
            </a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
  /* Optional highlight helper (legacy) */
  function toggleHighlight(tag, color) {
    const tgt = event.target;
    const els = document.querySelectorAll('.' + tag);
    const active = tgt.classList.contains('highlight');
    document.querySelectorAll('.highlight').forEach(e => e.classList.remove('highlight'));
    if (!active) {
      tgt.classList.add('highlight');
      els.forEach(e => e.style.backgroundColor = color);
    } else {
      els.forEach(e => e.style.backgroundColor = '');
    }
  }
</script>
</body>
</html>
