<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Improving Driving Safety Assessment via Object-Level Visual Prompting in Vision-Language Models</title>
  <link rel="stylesheet" href="static/css/bootstrap.min.css">
  <link rel="stylesheet" href="static/css/style.css">
</head>

<body>
  <div class="container" id="main">
    <div class="row">
      <div class="col-md-12 text-center">
        <h2>Improving Driving Safety Assessment via Object-Level Visual Prompting in Vision-Language Models</h2>
        <h4><em>Anonymous Authors</em></h4>
        <p><a href="https://github.com/gray311/VPD-Driver" target="_blank">[Project Page]</a>
        <a href="static/paper/VPD_LM.pdf" target="_blank">[Paper]</a></p>
      </div>
    </div>

    <div class="row section">
      <div class="col-md-12">
        <h3>Abstract</h3>
        <p>
          We present VPD-LM, a vision-language framework for assessing safety in autonomous driving through long-tail risky scenario mining. VPD-LM combines object-centric visual prompting, temporal grounding, and chain-of-thought supervision to reason about complex interactions between the ego vehicle and surrounding agents. We introduce a rule-based graph mining method to extract diverse risk-prone scenarios and construct a large-scale driving risky reasoning dataset to fine-tune VPD-LM. Our model outperforms state-of-the-art vision-language models (VLMs), including GPT-4o and InternVL3, on both the Driving Risky Benchmark and general driving tasks from DriveBench. The results highlight the importance of structured visual input and grounded reasoning for safety-critical scene understanding. Since our task focuses on offline scenario mining rather than real-time inference, VPD-LM achieves significant gains without the constraints of runtime latency.
        </p>
      </div>
    </div>
  </div>
</body>
</html>
